<!DOCTYPE html>
<html>
  <head>
    <title>Portfolio</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.tailwindcss.com"></script>
  </head>
  <body>
    <!-- Title -->
    <div class="container mx-auto p-10 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg">
      <p class="font-sans text-4xl font-bold text-center">CS280A Project 3: Face Morphing</p>
      <p class="font-sans text-xl mt-5 text-slate-500 text-center">by Junhua (Michael) Ma</p>
    </div>

    <!-- Part 1 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Defining Correspondences</p>
      <div class="border-t border-slate-600 mb-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-sans text-lg m-5 px-16 mb-10">
        <b>Labeling: </b>Correpondences between two images are defined by multiple pairs of points in the two images that correspond to the same feature. 
        Given a pair of images, correspondence points are labeled manually with a simple program created with <b>ginput</b> from Matplotlib library.
        The output is two sets of points stored in a file, with each line of the file consisting of points (x1, y1) and (x2, y2) representing the coordinates of a corresponding
        pair of points in image 1 and image 2. Additionally, for my setup, the four corners of the images are always automatically added as corresponding points,
        and the size of the two images are expected to be the same.<br /><br />
        <b>Triangulation: </b>Given an image and a set of points in the image (with all 4 corner points), triangulation can connect between the points to form triangles
        that cover the entire image. In other words, triangulation splits image into triangles with vertices from the point set. Delaunay 
        triangulation is an especially effective method of triangulation that results in more even and nice triangles. For this project, <b>scipy.spatial.Delaunay</b>
        function is used.
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
      <div class="flex flex-wrap mx-10 text-center gap-4">
        <div class="flex-1 bg-gray-200 p-4 rounded-md min-w-[500px]">
          <p class="pb-2 text-lg font-bold">Input: Me & George Clooney</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/cpd_inputs.jpg" />
          </div>
        </div>
        <div class="flex-1 bg-gray-100 p-4 rounded-md min-w-[500px]">
          <p class="pb-2 text-lg font-bold">Output: Correspondence Points (N = 78) & Triangulation</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/cpd_tri.jpg" />
          </div>
        </div>
      </div>
      <p class="font-sans text-lg text-slate-700 m- mt-10 px-20">
        <b>Note: </b>Triangulation is only computed once using the average of the two sets of corresponding points.
        The above result is obtained by simply displaying the same triangulation with different sets of points as vertices. 
        This approach ensures that the triangles are the same configuation which is helpful for the morphing process.
      </p> 
    </div>

    <!-- Part 2 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Computing Mid-Way Face</p>
      <div class="border-t border-slate-600 mb-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-sans text-lg m-5 px-16">
        Given two images and points of correspondence, the following overall steps are performed to obtain image morphing by a certain warp fraction <mark>WF</mark>  
        and dissolve fraction <mark>DF</mark> from 0 to 1 (in the case of mid-face which is morphing by half, <mark>WF = DF = 0.5</mark>)
      </p>
      <p class="font-sans text-lg mx-20 px-5 py-5 mb-5 bg-gray-200 text-indigo-800 rounded-lg">
        For input image pair (A, B), points of correspondence (P1, P2), and fractions (WF, DF): <br />
        → <b>Step 1:</b> Perform Delaunay triangulation over shape PM = <mark>P1 * (1 - WF) + P2 * WF</mark> to obtain triangulation T.<br />
        → <b>Step 2:</b> Obtain 3 sets of triangulations TA, TB, TM from T by plugging respective points P1, P2, PM as vertices.<br />
        → <b>Step 3:</b> Warping - For each triangle in TA and TB, perform inverse Affine transformation with corresponding triangle in TM to obtain warped images WA and WB.<br />
        → <b>Step 4:</b> Cross Dissolve using formula <mark>WA * (1 - DF) + WB * DF</mark> to obtain final output.
      </p>
      <p class="font-sans text-lg m-5 px-16">
        <b>Inverse Affine Transformation: </b>The main step of image warping process. The Affine transformation matrix from one triangle to another can be solved
        as solution to a linear equation using numpy.linalg.lstsq function. Specifically, given vertices (x1,y1), (x2,y2), (x3,y3) of the source triangle, and 
        vertices (x1',y1'), (x2',y2'), (x3',y3') of the destination triangle, can solve for (a, b, c, d, e, f) to obtain the corresponding Affine transformation matrix
        based on the following equation:
      </p>
      <img class="h-24 object-scale-down" src="images/affine_eqn.jpg" />
      <p class="font-sans text-lg m-5 mb-10 px-16">
        The transformation is performed as inverse by having source triangles from TM and destination triangles from TA and TB, even though the actual
        warping is from images A, B to the mean shape. <b>The overall idea is that for each pixel p' in the destination triangle, the inverse affine transformation is performed to 
        obtain a corresponding pixel p in the source triangle where its value (RGB) is copied into pixel p'.</b> 
        This is to ensure that all pixels p' in the destination morphed image are assigned a value to prevent holes.
        Since affine transformation may result in pixel p' that is not integer, bilinear intepolation is used to ensure a valid p for each p'.
        Additionally, the function <b>skimage.draw.polygon</b> is convenient for obtaining list of all pixel coordiantes within any trianular region given the coordinates of the vertices.
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
      <div class="flex flex-wrap mx-10 mb-5 text-center gap-4">
        <div class="flex-1 bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-72 object-scale-down rounded-md" src="images/warp_avg.jpg" />
          </div>
        </div>
      </div>
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-100 p-4 rounded-md w-1/2">
          <p class="pb-2 text-lg font-bold">Mid-Way Face</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/mid_face.jpg" />
          </div>
        </div>
      </div> 
    </div>

    <!-- Part 3 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Morph Sequence</p>
      <div class="border-t border-slate-600 mb-3"></div>
      
      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-sans text-lg m-5 px-16">
        Using the setup from before, to create a continuous morph sequence between two images is simply to set the fractions <mark>WF = DF = t/N</mark> 
        where N is the total number of frames (N = 45) for <mark>t = [0, N]</mark>. The initial frame at <mark>t = 0</mark> has <mark>WF = DF = 0</mark> and the final frame at <mark>t = 45</mark> has
        <mark>WF = DF = 1</mark>, which correspond to the original images (morphing to themselves, or otherwise hidden due to cross dissolve).
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md w-1/2">
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/me_to_george.gif" />
          </div>
        </div>
      </div> 
    </div>
    
    <!-- Part 4 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Mean Face of Population (FEI Dataset)</p>
      <div class="border-t border-slate-600 mb-3"></div>
      
      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-sans text-lg m-5 px-16">
        The <a class="text-blue-500 underline" href="https://fei.edu.br/~cet/facedatabase.html" target="_blank" rel="noopener noreferrer">FEI Face dataset</a> is used for this and some subsequent sections, which contains a set of 200 neutral closeup photos of different people and their correspondence points.
        An example image from the dataset with the annotated points are shown below (triangulation shown here is not relevant as only the triangulation from average shape is used instead). 
      </p>
      <img class="max-h-80 object-scale-down" src="images/fei_sample.jpg" />
      <p class="font-sans text-lg m-5 px-16">
        To compute population mean face, simply compute the mean shape from the correspondence points and warp all images into the mean shape, and then average all the warpped images together.
      </p> 
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>

      <p class="font-sans font-bold text-lg m-5 px-8">Population Mean</p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-100 p-4 rounded-md w-1/2">
          <div class="py-2 px-1 flex justify-center">
            <img class="min-h-80 object-scale-down rounded-md" src="images/pop_avg.jpg" />
          </div>
        </div>
      </div> 
      
      <p class="font-sans font-bold text-lg m-5 px-8">Warping Selections to Population Mean</p> 
      <div class="flex flex-wrap mx-10 mb-10 text-center gap-4">
        <div class="flex-1 bg-gray-200 p-4 rounded-md min-w-[500px]">
          <p class="pb-2 text-lg font-bold">FEI #1</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-72 object-scale-down rounded-md" src="images/warp_pop1.jpg" />
          </div>
        </div>
        <div class="flex-1 bg-gray-100 p-4 rounded-md min-w-[500px]">
          <p class="pb-2 text-lg font-bold">FEI #30 </p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-72 object-scale-down rounded-md" src="images/warp_pop2.jpg" />
          </div>
        </div>
      </div>
      <div class="flex flex-wrap mx-10 mb-10 text-center gap-4">
        <div class="flex-1 bg-gray-200 p-4 rounded-md min-w-[500px]">
          <p class="pb-2 text-lg font-bold">FEI #77</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-72 object-scale-down rounded-md" src="images/warp_pop3.jpg" />
          </div>
        </div>
        <div class="flex-1 bg-gray-100 p-4 rounded-md min-w-[500px]">
          <p class="pb-2 text-lg font-bold">FEI #140</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-72 object-scale-down rounded-md" src="images/warp_pop4.jpg" />
          </div>
        </div>
      </div>
      
      <p class="font-sans font-bold text-lg m-5 px-8">Warping Between Self and Population Mean</p> 
      <div class="flex flex-wrap mx-10 text-center gap-4">
        <div class="flex-1 bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/warp_self_pop.jpg" />
          </div>
        </div>
      </div>
    </div>

    <!-- Part 5 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Caricatures: Extrapolating From Mean (FEI Dataset)</p>
      <div class="border-t border-slate-600 mb-3"></div>
      
      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-sans text-lg m-5 px-16">
        By extrapolating from the population mean computed previously, a caricature can be produced to capture more or less of certain characteristics
        of the population. Given 2 images A, B and their correspondence points PA and PB, the extrapolated shape based on parameter alpha is computed as 
        <mark>PB * alpha + PA * (1 - alpha)</mark>. By changing alpha, the resulting shape would be closer to either image A or image B. 
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        For the following results, <mark>alpha < 0</mark> is closer to self, <mark>alpha > 1</mark> is closer to population mean.
      </p> 
      <div class="flex mx-10 text-center gap-4">
        <div class="flex-1 basis-1/3 bg-gray-200 p-4 rounded-md">
          <p class="pb-2 font-bold">Population Female Mean</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/pop_female_avg.jpg" />
          </div>
        </div>
        <div class="flex-1 basis-2/3 bg-gray-100 p-4 rounded-md">
          <p class="pb-2 font-bold">Caricature</p>
          <div class="flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/exp_pop_fm.jpg" />
          </div>
        </div>
      </div>
    </div>

    <!-- Part 6.1 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Bells & Whistles 1: Face Transformation With Morphing</p>
      <div class="border-t border-slate-600 mb-3"></div>
      
      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-mono text-lg font-bold mx-20 m-5 px-16 py-5 bg-indigo-200 rounded-lg">
        The goal is to transform my profile photo to look like French female.
      </p>
      <p class="font-sans text-lg m-5 px-16">
        The population mean for French female is obtained online.
        Three types of morphing are explored: shape only, appearance only, and both.
        The implementation uses the same image morphing setup
        described before simply with different fractions. For shape only, <mark>WF = 0.5, DF = 0.0</mark>. For appearance only, <mark>WF = 0.0, DF = 0.5</mark>.
        For both, <mark>WF = DF = 0.5</mark>.
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
      <div class="flex mx-10 text-center gap-4">
        <div class="flex-1 basis-1/4 bg-gray-200 p-4 rounded-md">
          <p class="pb-2 font-bold">French Female Mean</p>
          <div class="py-2 px-1 flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/fr_female_avg.jpg" />
          </div>
        </div>
        <div class="flex-1 basis-3/4 bg-gray-100 p-4 rounded-md">
          <p class="pb-2 font-bold">Results</p>
          <div class="flex justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/warp_self_fr.jpg" />
          </div>
        </div>
      </div>
    </div>

    <!-- Part 6.2 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Bells & Whistles 2: Face Morphing Music Video</p>
      <div class="border-t border-slate-600 mb-3"></div>
      
      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-mono text-lg font-bold mx-20 m-5 px-16 py-5 bg-indigo-200 rounded-lg">
      The goal is to create an image morph sequence music video using representative photos of all major Taylor Swift albums (one image per album) in chronological order. (Face Morphing Era Tour!)
      </p>
      <p class="font-sans text-lg m-5 px-16">
        The morphing video is basically just multiple morph sequences chained together. So instead of just two image and morphing from image 1 to image 2,
        there is now a list of images ims[]. The same morph sequence setup from before is used where the <mark>video_sequence = morph_sequence(ims[1], ims[2]) + 
        morph_sequence(ims[2], ims[3]) + ...</mark>. The resulting gif is then converted to MP4 video with reduced speed added background music in iMovie editor.
        All images are gathered and annotated manually (N = 67) by me.
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        Each image in the video correspond to albums in chronological order as follows: <b>Taylor Swift</b> (2006), <b>Fearless</b> (2008), <b>Speak Now</b> (2010), <b>Red</b> (2012), <b>1989</b> (2014),
        <b>Reputation</b> (2017), <b>Lover</b> (2019), <b>Folklore</b> (2020), <b>Evermore</b> (2020), <b>Midnights</b> (2022), <b>The Tortured Poets Department</b> (2024) <br /><br />
        Link: <a class="text-blue-500 underline" href="https://www.youtube.com/watch?v=IZeC8wPPtKQ" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=IZeC8wPPtKQ</a>
      </p>
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-violet-200 p-4 rounded-md w-1/2">
          <div class="py-2 px-1 flex justify-center">
            <iframe width="640" height="360" src="https://www.youtube.com/embed/IZeC8wPPtKQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div> 
    </div>

    <!-- Part 6.3 -->
    <div class="container mx-auto p-5 pb-12 m-5 my-10 border-solid border-2 border-indigo-200 rounded-md shadow-lg flex flex-col justify-center">
      <p class="font-sans text-2xl ml-3 mb-3 font-bold">Bells & Whistles 3: Caricatures with PCA (FEI Dataset)</p>
      <div class="border-t border-slate-600 mb-3"></div>
      
      <p class="font-sans text-xl text-slate-500 m-3 px-8">Method & Explanations</p>
      <p class="font-mono text-lg font-bold mx-20 m-5 px-16 py-5 bg-indigo-200 rounded-lg">
        The goal is to analyze the FEI dataset from the eigenspace using PCA and transform images to create caricatures by adjusting color and shape coordinates in eigenspace.
      </p>
      <p class="font-sans text-lg m-5 px-16">
        The overall steps performed are as follows:
      </p>
      <p class="font-sans text-lg mx-20 px-5 py-5 mb-5 bg-gray-200 text-indigo-800 rounded-lg">
        For all input images and points of correspondence from FEI dataset (neutral expression): <br />
        → <b>Step 1:</b> Compute the mean shape and warp all images into the mean shape.<br />
        → <b>Step 2:</b> Compute color eigenspace with PCA, obtain "eigenfaces".<br />
        → <b>Step 3:</b> Use top K eigenfaces as basis for the color eigenspace (dimension reduction), obtain transformation functions between images and coordinates in color eigenspace of dimension K.<br />
        → <b>Step 4:</b> Compute shape eigenspace with PCA, obtain "eigenshapes".<br />
        → <b>Step 5:</b> Use top K eigenshapes as basis for the shape eigenspace (dimension reduction), obtain transformation functions between image shapes and coordiantes in shape eigenspace of dimension K.<br /> 
        → <b>Step 6:</b> Combine color and shape eigenspaces to obtain the full eigenspace, and transformation functions between images and coordinate in eigenspace of dimension 2 * K.<br />
        → <b>Step 7:</b> For any image, simply modify the eigen coordinate to produce caricatures.<br /> 
      </p>

      <p class="font-sans text-lg m-5 px-16">
        <b>Computing Eigenspace with PCA: </b>Given M by N matrix A from N = 200 images in dataset, where each image of size (300, 250) is flattened into 1D array of size M = 75000.
        PCA is performed using Singular Value Decomposition on centered images (subtract mean image from each image), specifically the numpy.linalg.svd function to compute U, S, and Vt, as shown by the following equation:
      </p>
      <img class="h-36 object-scale-down" src="images/svd.jpg" />
      <p class="font-sans text-lg m-5 px-16">
        The original shape of U from SVD function is M by N, and each column of U is a "eigenface" for a total of N = 200 eigenfaces with same size as original flattened images of M = 75000. 
        This allows the eigenfaces to be displayed like regular images, each representing unique color characteristics within the population. 
        The weight or importance of each eigenface is determined by its corresponding eigenvalue in S, with higher eigenvalue meaning greater importance. 
        After picking top K eigenfaces, the resulting U would be M by K. Each column of Vt is the corresponding coordinate of dimension K in color eigenspace for each of the top K eigenfaces.<br /><br />
        For color eigenspace, all images are first warped to mean shape to help with alignment of colors to more accurately pick up shared color characteristics between images.
        For shape eigenspace, the points of correspondences are directly used for PCA.
      </p>

      <p class="font-sans text-lg m-5 px-16">
        <b>Transforming from Eigenspaces to Image (Reconstruction): </b>The goal is that given a eigen coordiate (dimension K) in eigenspace, obtain its original values related to the image. 
        For color eigenspace, the output would be image of mean shape and certain coloring.
        For shape eigenspace, the output would be points denoting certain shape. This can be simply achieved with the same equation above for SVD, where in this case the values for U, S, and Vt is known
        and we are solving for A. In this case, N = 1, so Vt contains exactly one eigen coordinate, which outputs exactly one flattened image of size M = 75000.
      </p>

      <p class="font-sans text-lg m-5 px-16">
        <b>Transforming from Image to Eigenspaces: </b>The goal is that given an image or a shape (as points of correspondence), obtain its eigen coordinate (dimension K) in the eigenspace.
        The equation for this transformation is simply the inverse of the previous equation as shown below:
      </p>
      <img class="h-36 object-scale-down" src="images/svd_inv.jpg" />
      <p class="font-sans text-lg m-5 px-16">
        Since we know S, U, and A, we can solve for Vt which is the coordinate in eigenspace. Again, N = 1 in this case, so outputs exactly one coordinate of size K.
      </p>

      <p class="font-sans text-lg m-5 px-16">
        <b>Eigen Coordinates: </b>Since PCA is performed on centered images, the origin or coordinate with all zeros would be the mean image. 
        The origin of color eigenspace is exactly the population mean image since all images are first warped to mean shape, and the color is also average color.
        The origin of shape eigenspace translates directly to points denoting the mean shape.
        <b>So overall eigen coordinate represents the distance from the mean, with bigger coordinate value means further from the mean.</b>
        The color and shape eigen coordinates can be adjusted separately to adjust the image's similarity to the mean in terms of color or shape.
      </p>
      <div class="border-t border-slate-300 mx-10 my-3"></div>

      <p class="font-sans text-xl text-slate-500 m-3 px-8 mb-5">Results</p>
     
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        All the following results are demonstrated on the first image from FEI dataset as shown below together with its warped to mean shape:
      </p> 
      <img class="max-h-72 object-scale-down mb-5" src="images/axis_shape_util.jpg" />
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The warped image is intentionally placed on the left for ease of comparison with the results shown below, since it's already at mean shape with shape eigen coordinate at 0.
        The original image would be a certain distance away from mean shape, which would be captured by its shape eigen coordinate.
      </p> 

      <p class="font-sans font-bold text-lg m-5 px-8">Color Eigenspace: Eigenfaces (K = 16)</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The top 16 eigenfaces are shown below with their eigenvalues.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/eigenface_top.jpg" /> 
            <img class="max-h-96 object-scale-down rounded-md" src="images/eigenface_bot.jpg" />
          </div>
        </div>
      </div>  

      <p class="font-sans font-bold text-lg m-5 px-8">Color Eigenspace: Axis (K = 16)</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The following shows the effect of different color eigen coordinates. After the color eigen coordinate is computed for image, it is multiplied by 
        different alpha values in range [-2, 2]. For alpha = 0, the resulting color is identical to population mean. For alpha = 1, the color is the reconstruction
        of the original image. Overall, as the coordinate goes farther from 0, the color is farther from the mean color.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/axis_color.jpg" /> 
          </div>
        </div>
      </div>  

      <p class="font-sans font-bold text-lg m-5 px-8">Color Eigenspace: Selected Axis (K = 16)</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        Instead of scaling the entire K dimensional color eigen coordinate by alpha, we can also only select particular axis to scale instead.
        An interesting outcome is shown below where only odd axis (1, 3, 5, 7, 9, 11, 13, 15) or only even axis (0, 2, 4, 6, 8, 10, 12, 14) are scaled by alpha, 
        and the resulting color looks more masculine for odd axis and feminine for even axis. 
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <p class="pb-2 font-bold">Odd Axis Scaled by Alpha</p>
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-64 object-scale-down rounded-md" src="images/axis_color_sel_odd.jpg" /> 
          </div>
        </div>
      </div>  
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-100 p-4 rounded-md">
          <p class="pb-2 font-bold">Even Axis Scaled by Alpha</p>
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-64 object-scale-down rounded-md" src="images/axis_color_sel_even.jpg" /> 
          </div>
        </div>
      </div>  

      <p class="font-sans font-bold text-lg m-5 px-8">Color Eigenspace: Testing Different K</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The following shows the effect of different K values, which is the dimensionality of the eigen coordinate and related to the number of eigenfaces included.
        With higher K, there are more details and characteristics. For lower K, the color feels more broad and lack details.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-64 object-scale-down rounded-md" src="images/axis_k.jpg" /> 
          </div>
        </div>
      </div> 

      <p class="font-sans font-bold text-lg m-5 px-8">Shape Eigenspace: Axis (K = 16)</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The following shows the effect of different shape eigen coordinates, following the same setup as color eigenvalues with alpha values in range [-2, 2]. 
        For alpha = 0, the resulting shape is identical to population mean. 
        For alpha = 1, the shape is the reconstruction of the original image. 
        Overall, as the coordinate goes farther from 0, the shape is farther from the mean shape.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-64 object-scale-down rounded-md" src="images/axis_shape.jpg" /> 
          </div>
        </div>
      </div>  

      <p class="font-sans font-bold text-lg m-5 px-8">Shape Eigenspace: Selected Axis (K = 16)</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        Only the selected axis (6, 7, 8, 9, 10, 11) of the shape eigen coordinate is scaled by alpha.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/axis_shape_sel.jpg" /> 
          </div>
        </div>
      </div> 

      <p class="font-sans font-bold text-lg m-5 px-8">Combined Eigenspace (K = 16)</p>  
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The color and shape coordinates can be combined to transform both color and shape of images from the mean simultaneously.
        The overall process is as follows:
      </p> 
      <p class="font-sans text-lg mx-20 px-5 py-5 mb-5 bg-gray-200 text-indigo-800 rounded-lg">
        Given an image: <br />
        → <b>Step 1:</b> Compute color eigen coordinate <mark>EIG_C</mark> and shape eigen coordinate <mark>EIG_S</mark> of image.<br />
        → <b>Step 2:</b> Reconstruct color <mark>RC</mark> from <mark>EIG_C * ALPHA_C</mark> and shape <mark>RS</mark> from <mark>EIG_S * ALPHA_S</mark>, where <mark>ALPHA_C</mark> and <mark>ALPHA_S</mark> are respective scaling factors for color and shape.<br />
        → <b>Step 3:</b> Warp <mark>RC</mark> (has mean shape) to shape <mark>RS</mark> to obtain final output.<br />
      </p>
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        For my setup, since I'm using scaling on all K dimensions of color or shape eigenspace at once by <mark>ALPHA_C</mark> or <mark>ALPHA_S</mark>, the following results are shown in terms of
        <mark>[ALPHA_C, ALPHA_S]</mark> as inputs. When <mark>ALPHA_C = ALPHA_S = 0</mark>, this is the origin which is the population mean. Changing only <mark>ALPHA_S</mark> would only affect the shape, and chaning only <mark>ALPHA_C</mark> would only affect color.
        There could be a lot more variations if different combinations of axis and different scaling factor for each axis is used which is not explored in this project.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-100 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/morph_eigen.jpg" /> 
          </div>
        </div>
      </div>  

      <p class="font-sans font-bold text-lg m-5 px-8">Combined Eigenspace (K = 100)</p> 
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The same setup as before but with higher K and different scaling factors.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/morph_eigen_high_k.jpg" /> 
          </div>
        </div>
      </div>  

      <p class="font-sans font-bold text-lg m-5 px-8">Combined Eigenspace: Randomized Interpolation</p> 
      <p class="font-sans text-lg mb-5 mx-5 px-16">
        The following images are generated from fixed <mark>ALPHA_C = 1.0</mark> (original color, no color scaling) and different randomized scaling in range [-1, 2] for each axis of shape eigen coordinate.
        The result shows a lot more variations when all K dimensions are not scaled uniformly by alpha.
      </p> 
      <div class="flex mx-10 mb-5 text-center justify-center gap-4">
        <div class="bg-gray-200 p-4 rounded-md">
          <div class="py-2 px-1 flex flex-col justify-center">
            <img class="max-h-96 object-scale-down rounded-md" src="images/morph_eigen_rand_top.jpg" /> 
            <img class="max-h-96 object-scale-down rounded-md" src="images/morph_eigen_rand_bot.jpg" />
          </div>
        </div>
      </div> 
    </div>
  </body>
</html>